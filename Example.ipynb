{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2441830b-a151-43aa-8f98-ca5482a1f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from spark_shap import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299f28d-28ac-44ce-9aab-c31722467c99",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "We shall use a regression dataset from scikit learn. We also include some categorical variables to show that our implementation works with categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a8b10d-f4b0-46f4-860c-47b66722b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_diabetes()\n",
    "\n",
    "data = pd.DataFrame(x['data'], columns=x['feature_names'])\n",
    "data['target'] = x['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f457df6-6845-4b70-bda8-7b8273923c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create one binary variable\n",
    "data['sex_cat'] = 'F'\n",
    "data.loc[data['sex'] < 0, ['sex_cat']] = 'M'\n",
    "\n",
    "data['sex'] = data['sex_cat']\n",
    "data = data.drop('sex_cat', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73844a72-f799-4887-a3fe-94cb3c25fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#revert age to create category (to show it also works with categories)\n",
    "data = data.replace({\"age\": dict(zip(sorted(data['age'].unique()), range(19,19+data['age'].nunique())))})\n",
    "data['age'] = pd.qcut(data['age'], 5).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa30066-6536-4ef6-b74a-67b8b79df8d0",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "We initialize and create the train/test set on spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c79513-b07f-4f17-9bd5-86c43921af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark environment\n",
    "spark = SparkSession.Builder().appName(\"Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e4a494-d5a2-4f73-bfeb-9132422a77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to spark dataframe\n",
    "data_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a1d3b5-40a6-4bc1-8b11-86dfbfb2185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01e4be-c50f-4863-a166-ecfa69cce274",
   "metadata": {},
   "source": [
    "# Train Random Forest Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658e399c-c21f-48f6-8b75-7d7627e2ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert age and sex from string to integer\n",
    "transformers_pipe = Pipeline(stages=[StringIndexer(inputCol=\"age\", outputCol=\"age_int\", handleInvalid='error'),\n",
    "                                     StringIndexer(inputCol=\"sex\", outputCol=\"sex_int\", handleInvalid='error')])\n",
    "    \n",
    "transformers_pipe = transformers_pipe.fit(train_df)\n",
    "\n",
    "#then create feature vector as input for model\n",
    "vectorizer = VectorAssembler(inputCols=['age_int', 'sex_int', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3c2bb9-7e86-4202-bc6e-63156fbb09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model to train\n",
    "rf_model = RandomForestRegressor(featuresCol='features', labelCol='target', predictionCol='prediction',\n",
    "                            minInstancesPerNode=1, impurity=\"variance\", seed=0, numTrees=50, maxDepth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3844f1b-d8a6-4b10-8664-60ca9e506e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "t_df = vectorizer.transform(transformers_pipe.transform(train_df)) #transform input into final feature vector\n",
    "rf_model = rf_model.fit(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed595a18-7bf0-44a4-9332-b620be8fe9db",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We shall compute RMSE metric to check if our model is somewhat good. We also compute the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4679281c-498e-4e20-a81f-2c5b7d485e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      " idx    name    score\n",
      "   2     bmi 0.289361\n",
      "   8      s5 0.247779\n",
      "   6      s3 0.100881\n",
      "   9      s6 0.095658\n",
      "   3      bp 0.068733\n",
      "   7      s4 0.068496\n",
      "   5      s2 0.041516\n",
      "   0 age_int 0.040309\n",
      "   4      s1 0.038783\n",
      "   1 sex_int 0.008483\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "def extract_feature_importance(feature_imp, feature_names):\n",
    "    \"\"\"\n",
    "    Convierte las feature importante en un dataframe de pandas\n",
    "    \"\"\"\n",
    "    varlist = pd.DataFrame(feature_names, columns = [\"name\"]).reset_index().rename(columns = {\"index\": \"idx\"})\n",
    "\n",
    "    #asocia nombre columna a su score\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: feature_imp[x])\n",
    "    varlist = varlist.sort_values('score', ascending=False)\n",
    "    return varlist  \n",
    "\n",
    "feature_imp = extract_feature_importance(rf_model.featureImportances, ['age_int', 'sex_int', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'])\n",
    "print(\"Feature importance:\\n{}\".format(feature_imp.to_string(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d38652-7c13-4b50-88b9-636e6f6259b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.75282146435476\n",
      "58.402904767375034\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "reg_eva = RegressionEvaluator(predictionCol='prediction', labelCol='target', metricName='rmse')\n",
    "\n",
    "# train\n",
    "print(reg_eva.evaluate(rf_model.transform(t_df)))\n",
    "\n",
    "# test\n",
    "n_df = rf_model.transform(vectorizer.transform(transformers_pipe.transform(test_df)))\n",
    "print(reg_eva.evaluate(n_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd1183-0927-49b3-8f5e-8709fe033d1d",
   "metadata": {},
   "source": [
    "The model overfits a little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922f54f-5a2b-4d5c-b9a7-044dd5f8b368",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Study individual results using SHAP\n",
    "\n",
    "We shall consider a random test example to analize the effect of each feature on its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985709a4-9c97-4a3e-93ef-8625a4fbd554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select random test example\n",
    "row = n_df.limit(1).toPandas().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d386b18-4346-4669-9d92-acf8b4ceb6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                                              (18.999, 36.0]\n",
      "sex                                                           F\n",
      "bmi                                                   -0.022373\n",
      "bp                                                     0.001215\n",
      "s1                                                    -0.037344\n",
      "s2                                                    -0.026366\n",
      "s3                                                     0.015505\n",
      "s4                                                    -0.039493\n",
      "s5                                                    -0.072128\n",
      "s6                                                    -0.017646\n",
      "target                                                     49.0\n",
      "age_int                                                     1.0\n",
      "sex_int                                                     1.0\n",
      "features      [1.0, 1.0, -0.0223731352440218, 0.001215130832...\n",
      "prediction                                            98.684199\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498e24e-4c27-4cf1-8efd-7a9c183eedd0",
   "metadata": {},
   "source": [
    "- For SHAP we first estimate the mean prediction\n",
    "- Then all the computed features should add to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ab05b75-46f0-4029-84a8-626c6d2aef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.40146099831344\n"
     ]
    }
   ],
   "source": [
    "avg_val = n_df.select(F.avg('prediction')).toPandas().iloc[0][0]\n",
    "print(avg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de6fc9-a552-4239-ad05-32dabc20aeff",
   "metadata": {},
   "source": [
    "- SHAP values should be computed with the test dataset\n",
    "- SHAP works with monte carlo sampling, so we shall duplicate the test_df by some factor, to have more examples to sample. The more examples, the more exact our approximation will be to the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8915a4fb-6856-4d13-95d2-13d0c926c158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3003\n"
     ]
    }
   ],
   "source": [
    "def duplicate_df(df, n):\n",
    "    \"\"\"\n",
    "    Duplicate dataset for more accurate monte carlo sampling\n",
    "    Since we are duplicating the average will not change\n",
    "    \"\"\"\n",
    "    \n",
    "    res = df\n",
    "    for _ in range(n):\n",
    "        res = res.union(df)\n",
    "        \n",
    "    return res\n",
    "\n",
    "entire_df = duplicate_df(n_df.drop('target','features','prediction'), 20)\n",
    "print(entire_df.persist().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1360a61-b454-4510-88ae-ed28622c5a1f",
   "metadata": {},
   "source": [
    "We shall compute shap for some variables. For this example we shall ignore the two variables with the smallest feature importance. \n",
    "\n",
    "Note that you can compute the SHAP values for all variables, but the more variables you insert, the bigger the sampling data should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c72ad8e-7431-4dab-9bb7-ffd0d80e8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['age_int', 'sex_int', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "important_features = feature_imp['name'].values[:-2].tolist() #we want to see if the more important features are the ones with a bigger impact on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa97f7d7-87a1-4d0c-af8a-35aa4beedcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import spark_shap\n",
    "importlib.reload(spark_shap)\n",
    "from spark_shap import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d475650e-a19e-481d-a6eb-8ec5732d0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmi:-15.656472827859787\n",
      "s5:-20.949190445891805\n",
      "s3:-1.5008425770699845\n",
      "s6:-4.155754621024916\n",
      "bp:-2.265371745520624\n",
      "s4:-5.5344540853024995\n",
      "s2:-0.4375498302434104\n",
      "age_int:1.4347488888394524\n"
     ]
    }
   ],
   "source": [
    "shap_values = compute_shap_values(entire_df, \n",
    "                                  row, \n",
    "                                  all_features, \n",
    "                                  important_features, \n",
    "                                  prediction_fun = lambda df: rf_model.transform(df), \n",
    "                                  features_col='features', \n",
    "                                  prediction_col='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a6fe8-4f5b-4b0e-879f-5c82450194e2",
   "metadata": {},
   "source": [
    "The goal of computing the shap values is that the sum of all shap values should equate the value predicted for that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bafe954-ad3b-45ee-bdaa-8cd8c590b1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.33657375423986"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum of shap values\n",
    "avg_val + sum(shap_values.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97f8dd3-236d-427b-95f5-e701844633b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.68419853727168"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction of row being analized\n",
    "row['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946f360-d8fb-4477-8f7c-050358178922",
   "metadata": {},
   "source": [
    "Due to monte carlo sampling, the values will always be an approximation. But the sum is close enough to the actual value, so we can explain the impact of each important feature of our Spark Random Forest Regression on the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f15b21b-03f3-42d9-89b8-cdbffbcdd311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s5', -20.949190445891805), ('bmi', -15.656472827859787), ('s4', -5.5344540853024995), ('s6', -4.155754621024916), ('bp', -2.265371745520624), ('s3', -1.5008425770699845), ('s2', -0.4375498302434104), ('age_int', 1.4347488888394524)]\n"
     ]
    }
   ],
   "source": [
    "#order values by importance\n",
    "print(sorted( ((k,v) for k,v in shap_values.items()), key=lambda x: x[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748efb4-1c3e-4ffc-b785-682dba9a8ae0",
   "metadata": {},
   "source": [
    "We can see in this case that *bmi* and *s5* are the features with the biggest impact on the prediction of this example row and also the features with the biggest feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196255a-19ac-44ab-89b3-d9bb50ec3bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
